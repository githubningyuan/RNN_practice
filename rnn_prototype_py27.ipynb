{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   lengths summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     492\n",
       "4     451\n",
       "5     380\n",
       "6     318\n",
       "7     292\n",
       "8     278\n",
       "9     200\n",
       "10    169\n",
       "11    155\n",
       "12    116\n",
       "13     89\n",
       "14     99\n",
       "15     89\n",
       "16     62\n",
       "17     42\n",
       "18     43\n",
       "19     18\n",
       "20     14\n",
       "21      7\n",
       "22      1\n",
       "25      2\n",
       "Name: length, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "#from datetime import datetime, date\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpath_id = './save_features/other/0_uid.csv'\n",
    "df_id = pd.read_csv(fpath_id, header=None)\n",
    "df_id.columns = ['uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000_forcast_diab.csv', '0_uid.csv', '10_earlobe.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_list = os.listdir('./save_features/other')\n",
    "other_list.sort()\n",
    "other_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_yr = pd.read_csv('./save_features/other/4_yr.csv', header=None)\n",
    "df_yr.columns = ['yr']\n",
    "df_mon = pd.read_csv('./save_features/other/5_mon.csv', header=None)\n",
    "df_mon.columns = ['mon']\n",
    "df_day = pd.read_csv('./save_features/other/6_day.csv', header=None)\n",
    "df_day.columns = ['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25142, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>mon</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     yr  mon  day\n",
       "0  2005    3   12\n",
       "1  2006   10   21"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date = pd.concat([df_yr, df_mon, df_day], axis = 1)\n",
    "print(df_date.shape)\n",
    "df_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_series = pd.to_datetime(dict(year=df_date.yr, month=df_date.mon, day=df_date.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>del_yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   del_yr\n",
       "0     0.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_series = pd.to_datetime(dict(year=df_date.yr, month=df_date.mon, day=df_date.day))\n",
    "#365*24*60*60 = 31536000\n",
    "df_dt_365 = pd.DataFrame(date_series.diff().dt.total_seconds().fillna(0)/31536000.)\n",
    "df_dt_365.columns = ['del_yr']\n",
    "df_dt_365.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>del_yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   del_yr\n",
       "0     0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dt_365[  (df_id.shift(1)!=df_id)['uid']    ] = 0\n",
    "df_dt_365.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "df_forcast_365 = df_dt_365.shift(-1)\n",
    "df_forcast_365['del_yr'][len(df_forcast_365)-1] = 0\n",
    "df_forcast_365.columns = ['forcast_yr']\n",
    "print( df_forcast_365['forcast_yr'][len(df_forcast_365)-1]   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_dt_365.to_csv('./save_features/numerical/00_del_yr.csv', index=False,  header=None)\n",
    "df_forcast_365.to_csv('./save_features/numerical/000_forcast_yr.csv', \n",
    "                      index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpath_ans = './save_features/other/3_diabetes.csv'\n",
    "df_ans = pd.read_csv(fpath_ans, header=None)\n",
    "df_ans.columns = ['diabetes'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_forcast_diabetes = df_ans.shift(-1)\n",
    "df_forcast_diabetes[  (df_id.shift(-1)!= df_id)['uid']    ] = df_ans[  (df_id.shift(-1)!= df_id)['uid']    ]\n",
    "df_forcast_diabetes['diabetes'][len(df_forcast_diabetes) - 1] = df_ans['diabetes'][len(df_forcast_diabetes) - 1]\n",
    "df_forcast_diabetes.columns = ['forcast_diab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_forcast_diabetes.to_csv('./save_features/other/0000_forcast_diab.csv',\n",
    "                           index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3275 samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.442413806915283"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "#import datetime\n",
    "#from datetime import datetime, date\n",
    "import time\n",
    "import os\n",
    "t1 = time.time()\n",
    "\n",
    "fpath_id = './save_features/other/0_uid.csv'\n",
    "df_id = pd.read_csv(fpath_id, header=None)\n",
    "df_id.columns = ['uid']\n",
    "\n",
    "#fpath_ans = './save_features/other/3_diabetes.csv'\n",
    "fpath_ans = './save_features/other/0000_forcast_diab.csv'\n",
    "df_ans = pd.read_csv(fpath_ans, header=None)\n",
    "# df_ans.columns = ['diabetes'] \n",
    "df_ans.columns = ['forcast_diab'] \n",
    "\n",
    "#df_tar = pd.get_dummies(df_ans['diabetes'])\n",
    "df_tar = pd.get_dummies(df_ans['forcast_diab'])\n",
    "df_tar.columns = ['no','yes']\n",
    "\n",
    "fpath_num = './save_features/numerical/'\n",
    "fpath_cate = './save_features/categorical/'\n",
    "\n",
    "num_list = os.listdir(fpath_num)[:50]\n",
    "cate_list = os.listdir(fpath_cate)[:30]\n",
    "\n",
    "df_n = pd.read_csv(fpath_num+num_list[0], header=None)\n",
    "df_n.columns = [num_list[0][4:-4]] ### needs regular expression\n",
    "for name in num_list[1:]:\n",
    "    df_n_new = pd.read_csv(fpath_num + name, header=None)\n",
    "    df_n_new.columns = [name[4:-4]]\n",
    "    df_n = pd.concat([df_n,  df_n_new], axis=1)\n",
    "    \n",
    "df_c = pd.read_csv(fpath_cate+cate_list[0])#, header=None)\n",
    "#df_c.columns = [cate_list[0][4:-4]] ### needs regular expression\n",
    "cate_ft_len = [len(df_c.columns)]\n",
    "for name in cate_list[1:]:\n",
    "    df_c_new = pd.read_csv(fpath_cate + name)#, header=None)\n",
    "    cate_ft_len.append( len(df_c_new.columns)  )\n",
    "    #df_c_new.columns = [name[4:-4]]\n",
    "    df_c = pd.concat([df_c,  df_c_new], axis=1)\n",
    "    \n",
    "num_ft_len = [len(df_n.columns)]\n",
    "ft_len_list = num_ft_len + cate_ft_len\n",
    "\n",
    "df_total = pd.concat([df_n,  df_c], axis=1)\n",
    "\n",
    "#######  df_len  should be saved and loaded, not calculated everytime!!\n",
    "length = []\n",
    "for id_ in set(df_id.uid):\n",
    "    length.append(len(df_id[df_id['uid']==id_]))\n",
    "df_len_1 = pd.DataFrame(list(set(df_id.uid)))\n",
    "df_len_1.columns = ['uid'] \n",
    "df_len_2 = pd.DataFrame(length)\n",
    "df_len_2.columns = ['length']\n",
    "df_len = pd.concat([df_len_1,  df_len_2], axis=1)\n",
    "\n",
    "batch_list = []\n",
    "for ii in range(26):\n",
    "    batch_list.append([])\n",
    "for len_ in range(3, 23):\n",
    "    batch_list[len_].append( df_len[df_len.length==len_]    )\n",
    "batch_list[25].append( df_len[df_len.length==25]    )\n",
    "\n",
    "batches = []\n",
    "targets = []\n",
    "for ii in range(26):\n",
    "    batches.append([])\n",
    "    targets.append([])    \n",
    "for idx_len in range(3,19):\n",
    "    for id_ in batch_list[idx_len][0]['uid']:\n",
    "        batches[idx_len].append(np.array(  df_total[df_id['uid'] == id_] ) )\n",
    "        targets[idx_len].append(np.array(  df_ans[df_id['uid'] == id_] ) )   \n",
    "\n",
    "\n",
    "record = df_len['length'].value_counts().sort_index()\n",
    "num_samples = 0\n",
    "max_len = 18\n",
    "for idx in range(3, max_len+1):\n",
    "    num_samples += record[idx]\n",
    "print('%d samples.'%num_samples)\n",
    "\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1.0\n",
    "MAX_GRAD_NORM = 1\n",
    "KEEP_PROB = 0.8\n",
    "\n",
    "max_len = 18\n",
    "#NUM_FT = np.array(batches[3]).shape[2]  ## NUM_FT = 50\n",
    "NUM_FT = sum(ft_len_list)\n",
    "###  ft_len_list  = num_ft_len + cate_ft_len\n",
    "\n",
    "HIDDEN_SIZE = 50\n",
    "TAR_SIZE = 1\n",
    "CLASS_SIZE = 2\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "NUM_EPOCH = 500\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TRAIN_NUM_STEP = max_len\n",
    "epoch_size = int(num_samples/TRAIN_BATCH_SIZE )\n",
    "\n",
    "EVAL_BATCH_SIZE = 1\n",
    "EVAL_NUM_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3275, 18, 149), (3275, 18, 1), (3275,))\n"
     ]
    }
   ],
   "source": [
    "train_X = np.zeros([num_samples, max_len, NUM_FT])\n",
    "train_Y = np.zeros([num_samples, max_len, 1])\n",
    "train_lengths = np.zeros([num_samples])\n",
    "\n",
    "idx_i = 0\n",
    "for idx in range(3, max_len+1):\n",
    "    idx_f = idx_i + record[idx]\n",
    "    train_X[idx_i: idx_f, :idx, :] = np.array(  batches[idx]  )\n",
    "    train_Y[idx_i: idx_f, :idx, :] = np.array(  targets[idx]  )\n",
    "    train_lengths[idx_i: idx_f] = idx\n",
    "    idx_i = idx_f\n",
    "\n",
    "print(train_X.shape, train_Y.shape, train_lengths.shape)\n",
    "\n",
    "### Shuffle:\n",
    "indices = np.array(range(num_samples) )\n",
    "np.random.shuffle(indices)\n",
    "train_X = train_X[indices]\n",
    "train_Y = train_Y[indices]\n",
    "train_lengths = train_lengths[indices]\n",
    "\n",
    "train_data = [train_X, train_Y, train_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_batch(raw_data, batch_size, num_steps, NUM_FT, epoch_size):  \n",
    "    data_x = tf.convert_to_tensor(raw_data[0], name=\"raw_data\",\n",
    "                                      dtype=tf.float32)\n",
    "    data_y = tf.convert_to_tensor(raw_data[1], name=\"raw_data\",\n",
    "                                      dtype=tf.int32)  \n",
    "    data_l = tf.convert_to_tensor(raw_data[2], name=\"raw_data\",\n",
    "                                      dtype=tf.int32)  \n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data_x, [i*batch_size, 0, 0],\n",
    "                         [(i + 1)*batch_size, num_steps, NUM_FT], [1,1,1])\n",
    "    y = tf.strided_slice(data_y, [i*batch_size, 0, 0], \n",
    "                         [(i + 1)*batch_size, num_steps, 1], [1,1,1])\n",
    "    l = tf.strided_slice(data_l, [i*batch_size], \n",
    "                         [(i + 1)*batch_size], [1])\n",
    "    x.set_shape([batch_size, num_steps, NUM_FT])\n",
    "    y.set_shape([batch_size, num_steps, 1])\n",
    "    l.set_shape([batch_size])\n",
    "    \n",
    "    return x, y, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, batch_data, train_op, output_log, epoch_size):\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    for step in range(epoch_size):\n",
    "        x, y, length = session.run(batch_data)\n",
    "        cost, state, _ = session.run([model.cost,\n",
    "                                      model.final_state,\n",
    "                                      train_op],\n",
    "                                   feed_dict ={model.input_data: x,\n",
    "                                    model.targets: y,\n",
    "                                    model.seq_len: length })\n",
    "                                   # model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        #if output_log and step % 40  == 0:\n",
    "            #print(\"After %d steps, loss is %.3f\" % (step, total_costs / iters))\n",
    "            #print(\"After %d steps, perplexity is %.3f\" % (step, np.exp(total_costs / iters)))\n",
    "    return np.exp(total_costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Diab_Model(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps,\n",
    "                                   ft_size, tar_size, HIDDEN_SIZE):\n",
    "        '''\n",
    "        ** ft_size is a list,like this:\n",
    "         [100, 3, 2, 4, ...]. ft_len_list[0] is number of numerical features,\n",
    "         ft_len_list[1:] correspond to number of class for each one-hot encoding feature.  \n",
    "        '''\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        NUM_FT = sum(ft_size)\n",
    "        self.n_ft_size = ft_size[0]\n",
    "        self.tar_size = tar_size \n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "\n",
    "        ## Define LSTM layers, use \"Dropout\" when training:\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell( self.hidden_size  )\n",
    "        #lstm_cell = tf.contrib.rnn.BasicLSTMCell( self.hidden_size )        \n",
    "        if is_training:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, \n",
    "                                        output_keep_prob=KEEP_PROB)\n",
    "            #lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "            #                            output_keep_prob=KEEP_PROB)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell]*NUM_LAYERS)\n",
    "        #cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*NUM_LAYERS)\n",
    "\n",
    "        ## Set weights and bias:\n",
    "        self.n_ft_weight = tf.get_variable(\"n_ft_weight\", [self.n_ft_size, self.hidden_size])\n",
    "        self.n_ft_bias = tf.get_variable(\"n_ft_bias\", [self.hidden_size])\n",
    "        \n",
    "        self.weight = tf.get_variable(\"weight\", [self.hidden_size, CLASS_SIZE])\n",
    "        self.bias = tf.get_variable(\"bias\", [CLASS_SIZE])\n",
    "        \n",
    "        '''\n",
    "        weights: coding here\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        ## Set inputs:\n",
    "        #inputs = self.input_data\n",
    "        self.input_data = tf.placeholder(tf.float32, \n",
    "                                [batch_size, num_steps, NUM_FT ])\n",
    "        self.targets = tf.placeholder(tf.int32, \n",
    "                                [batch_size, num_steps, tar_size ])\n",
    "        self.seq_len = tf.placeholder(tf.int32, [batch_size])\n",
    "        self.initial_state = cell.zero_state(\n",
    "                             batch_size, tf.float32)\n",
    "#        self.initial_state = cell.zero_state(\n",
    "#                          [batch_size], tf.float32)\n",
    "        states = self.get_state_variables(batch_size, cell)\n",
    "        \n",
    "        '''\n",
    "        inputs = tf.reshape( self.input_data, [-1, ft_size])\n",
    "        inputs = tf.matmul(inputs, self.n_ft_weight) + self.n_ft_bias\n",
    "        inputs = tf.reshape( inputs, [batch_size, num_steps, self.hidden_size])\n",
    "        '''\n",
    "        n_inputs = tf.strided_slice(self.input_data, [0, 0, 0],\n",
    "                     [batch_size, num_steps, self.n_ft_size], [1,1,1])\n",
    "        self.n_inputs  = tf.reshape( n_inputs , [-1, self.n_ft_size])\n",
    "        self.n_latent  = tf.matmul(self.n_inputs , self.n_ft_weight) + self.n_ft_bias\n",
    "        #n_latent  = tf.reshape(n_latent, [batch_size, num_steps, self.hidden_size])\n",
    "        \n",
    "        self.c_inputs = tf.strided_slice(self.input_data, [0, 0, self.n_ft_size],\n",
    "                     [batch_size, num_steps, NUM_FT], [1,1,1])\n",
    "        \n",
    "        c_weights = []\n",
    "        for idx in range(1, len(ft_size) ) :\n",
    "            embed_sz = ft_size[idx]/2\n",
    "            name = 'c_weight_' + str(idx)\n",
    "            c_weights.append( self.get_weights(\n",
    "                             ft_size[idx], embed_sz, name) )  \n",
    "\n",
    "        c_rk2_inputs = tf.reshape(self.c_inputs , [-1, NUM_FT-self.n_ft_size])\n",
    "        c_one_input = tf.strided_slice(c_rk2_inputs, [0, 0],\n",
    "                            [batch_size*num_steps, ft_size[1] ], [1,1])\n",
    "        c_latent = tf.matmul(c_one_input, c_weights[0][0] ) + c_weights[0][1] \n",
    "        idx_i = ft_size[1]\n",
    "        \n",
    "        for idx in range(2, len(ft_size) ):\n",
    "            idx_f = idx_i + ft_size[idx]\n",
    "            c_one_input = tf.strided_slice(c_rk2_inputs, [0, idx_i],\n",
    "                            [batch_size*num_steps, idx_f ], [1,1])\n",
    "            c_one_latent = tf.matmul(c_one_input, c_weights[idx-1][0] \n",
    "                                ) + c_weights[idx-1][1]  \n",
    "            idx_i = idx_f\n",
    "            # c_latent = tf.concat([c_latent, c_one_latent], axis=1) ##for py35\n",
    "            c_latent = tf.concat(1, [c_latent, c_one_latent]) ##for py27\n",
    "            \n",
    "        self.c_latent = c_latent  \n",
    "\n",
    "        #c_latent  = tf.reshape(c_latent, [batch_size, num_steps,\n",
    "        #                                  tf.shape(c_latent)[1]])\n",
    "\n",
    "        #total_inputs = tf.concat([n_latent, c_latent], axis=1) ##for py35\n",
    "        total_inputs = tf.concat(1, [self.n_latent, c_latent]) ##for py27\n",
    "        total_inputs = tf.reshape(total_inputs,\n",
    "                    [batch_size, num_steps, tf.shape(total_inputs)[1]] )\n",
    "  \n",
    "        if is_training:\n",
    "            total_inputs = tf.nn.dropout(total_inputs, KEEP_PROB)\n",
    "\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "#            for time_step in range(num_steps):\n",
    "#                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "            #tf.get_variable_scope().reuse_variables()  ## ????\n",
    "    \n",
    "            cell_output, state = tf.nn.dynamic_rnn( cell,\n",
    "                                   total_inputs, self.seq_len,\n",
    "                                   initial_state = states)\n",
    "        \n",
    "        output = tf.reshape( cell_output, [-1, self.hidden_size])  \n",
    "        self.logits = tf.matmul(output, self.weight) + self.bias\n",
    "        \n",
    "        ## Define loss and mean cost:\n",
    "        #loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [self.logits],\n",
    "            [tf.reshape(self.targets, [-1])],\n",
    "            [tf.ones([batch_size * num_steps ], dtype=tf.float32)])\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "                \n",
    "        if not is_training: return\n",
    "    \n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        \n",
    "    def get_state_variables(self, batch_size, cell):\n",
    "        # For each layer, get the initial state and make a variable out of it\n",
    "        # to enable updating its value.\n",
    "        state_variables = []\n",
    "        for state_c, state_h in cell.zero_state(batch_size, tf.float32):\n",
    "#            state_variables.append(tf.contrib.rnn.LSTMStateTuple(\n",
    "            state_variables.append(tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                tf.Variable(state_c, trainable=False),\n",
    "                tf.Variable(state_h, trainable=False)))\n",
    "        # Return as a tuple, so that it can be fed to dynamic_rnn as an initial state\n",
    "        return tuple(state_variables) \n",
    "    \n",
    "    def get_weights(self, input_dim, output_dim, name):\n",
    "        name_w = name + '_w'\n",
    "        name_b = name + '_b'\n",
    "        weight = tf.get_variable(name_w, [input_dim, output_dim])\n",
    "        bias = tf.get_variable(name_b, [output_dim])\n",
    "        return [weight, bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 0\n",
      "perplexity is 1.424\n",
      "In iteration: 20\n",
      "perplexity is 1.336\n",
      "In iteration: 40\n",
      "perplexity is 1.304\n",
      "In iteration: 60\n",
      "perplexity is 1.275\n",
      "In iteration: 80\n",
      "perplexity is 1.268\n",
      "In iteration: 100\n",
      "perplexity is 1.263\n",
      "In iteration: 120\n",
      "perplexity is 1.258\n",
      "In iteration: 140\n",
      "perplexity is 1.268\n",
      "In iteration: 160\n",
      "perplexity is 1.255\n",
      "In iteration: 180\n",
      "perplexity is 1.250\n",
      "In iteration: 200\n",
      "perplexity is 1.250\n",
      "In iteration: 220\n",
      "perplexity is 1.253\n",
      "In iteration: 240\n",
      "perplexity is 1.248\n",
      "In iteration: 260\n",
      "perplexity is 1.248\n",
      "In iteration: 280\n",
      "perplexity is 1.259\n",
      "In iteration: 300\n",
      "perplexity is 1.246\n",
      "In iteration: 320\n",
      "perplexity is 1.249\n",
      "In iteration: 340\n",
      "perplexity is 1.245\n",
      "In iteration: 360\n",
      "perplexity is 1.246\n",
      "In iteration: 380\n",
      "perplexity is 1.246\n",
      "In iteration: 400\n",
      "perplexity is 1.244\n",
      "In iteration: 420\n",
      "perplexity is 1.247\n",
      "In iteration: 440\n",
      "perplexity is 1.244\n",
      "In iteration: 460\n",
      "perplexity is 1.249\n",
      "In iteration: 480\n",
      "perplexity is 1.244\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "with tf.variable_scope(\"diab_model\", reuse=None, initializer=initializer):\n",
    "    train_model = Diab_Model(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP,\n",
    "                                 ft_len_list, TAR_SIZE, HIDDEN_SIZE)\n",
    "with tf.variable_scope(\"diab_model\", reuse=True, initializer=initializer):\n",
    "    eval_model = Diab_Model(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP, \n",
    "                                 ft_len_list, TAR_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "session = tf.Session()\n",
    "#session.run( tf.global_variables_initializer()  )\n",
    "session.run( tf.initialize_all_variables()  )\n",
    "train_queue = one_batch(train_data, TRAIN_BATCH_SIZE,\n",
    "                            TRAIN_NUM_STEP, NUM_FT, epoch_size)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "for i in range(NUM_EPOCH):\n",
    "    #t1 = time.time()\n",
    "    perplexity = run_epoch(session, train_model, train_queue,\n",
    "                            train_model.train_op, True, epoch_size)\n",
    "    #del_t = time.time()-t1\n",
    "    if i%20 ==0:\n",
    "        print(\"In iteration: %d\" % (i) )\n",
    "        print(\"perplexity is %.3f\" % (perplexity))\n",
    "        #print('time spent is %.3f secs' % (del_t))\n",
    "            \n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14149956227369942"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1.152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3275, 18, 149)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.95342466,   1.58356164,   1.15068493,   1.34246575,\n",
       "          1.59178082,   1.09315068,   0.95890411,   1.80273973,\n",
       "          1.03561644,   0.92054795,  10.        ,  10.        ,\n",
       "          1.01643836,   1.16986301,   0.8630137 ,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[11:12, :, 47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3275, 18, 149), (3275, 18, 1), (3275,))\n"
     ]
    }
   ],
   "source": [
    "x = train_X[11:12, 8:9, :]\n",
    "y = train_Y[11:12, 8:9, :]\n",
    "length = train_lengths[11:12,]\n",
    "print(train_X.shape, train_Y.shape, train_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'000_forcast_yr.csv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(  len(num_list)   )\n",
    "num_list[47]  ###  '000_forcast_yr.csv'\n",
    "#cate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.36558904,  0.05432127]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,47] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.95342466,   1.58356164,   1.15068493,   1.34246575,\n",
       "          1.59178082,   1.09315068,   0.95890411,   1.80273973,\n",
       "          1.03561644,   0.92054795,  10.        ,  10.        ,\n",
       "          1.01643836,   1.16986301,   0.8630137 ,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[11:12, :, 47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 149)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0,47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.10000000000000001, 0.17435222699496827)\n",
      "(0.59999999999999998, 0.17435222699496827)\n",
      "(1.1000000000000001, 0.17435222699496827)\n",
      "(1.6000000000000001, 0.24390848686513664)\n",
      "(2.1000000000000001, 0.24390848686513664)\n",
      "(2.6000000000000001, 0.24390848686513664)\n",
      "(3.1000000000000001, 0.24390848686513664)\n",
      "(3.6000000000000001, 0.32916636615605105)\n",
      "(4.0999999999999996, 0.32999470662024943)\n",
      "(4.5999999999999996, 0.32999470662024943)\n",
      "(5.0999999999999996, 0.32999470662024943)\n",
      "(5.5999999999999996, 0.47996865439085706)\n",
      "(6.0999999999999996, 0.47996865439085706)\n",
      "(6.5999999999999996, 0.47996865439085706)\n",
      "(7.0999999999999996, 0.47996865439085706)\n",
      "(7.5999999999999996, 0.54865419576071084)\n",
      "(8.0999999999999996, 0.59204466239632114)\n",
      "(8.5999999999999996, 0.59204466239632114)\n",
      "(9.0999999999999996, 0.59204466239632114)\n",
      "(9.5999999999999996, 0.67278393441361406)\n"
     ]
    }
   ],
   "source": [
    "for idx in range(20):\n",
    "    x[0,0,47] = 0.1 + idx* 0.5\n",
    "    state, logits = session.run([eval_model.final_state, eval_model.logits],\n",
    "                            feed_dict ={\n",
    "                            eval_model.input_data: x,\n",
    "                            eval_model.targets: y,\n",
    "                            eval_model.seq_len: length })\n",
    "    softmax_ = 1./(np.exp(logits[0][0] - logits[0][1])  +1.    )\n",
    "    print(x[0,0,47], softmax_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#state = session.run(eval_model.initial_state)\n",
    "#train_data = [train_X, train_Y, train_lengths]\n",
    "state, logits = session.run([eval_model.final_state, eval_model.logits],\n",
    "                            feed_dict ={\n",
    "                            eval_model.input_data: x,\n",
    "                            eval_model.targets: y,\n",
    "                            eval_model.seq_len: length })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45776057,  0.48654795]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_9:0' shape=(84, 110) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = tf.placeholder(tf.float32, [12, 7, 60 ])\n",
    "c_weights = []\n",
    "for idx in range(10):\n",
    "    name = 'c_weight_' + str(idx)\n",
    "    c_weights.append(  get_weights(6, 11, name) )  \n",
    "\n",
    "bbb = tf.reshape(aaa , [-1, 60])\n",
    "xxx = tf.strided_slice(bbb, [0, 0], [12*7, 6], [1,1])\n",
    "zzz = tf.matmul(xxx, c_weights[0][0] ) + c_weights[0][1] \n",
    "\n",
    "for idx in range(1, 10):\n",
    "    xxx = tf.strided_slice(bbb, [0, idx*6], [12*7, (idx+1)*6], [1,1])\n",
    "    yyy = tf.matmul(xxx, c_weights[idx][0] ) + c_weights[idx][1] \n",
    "    # zzz = tf.concat([zzz, yyy], axis=1) ##for py35\n",
    "    zzz = tf.concat(1, [zzz, yyy])   ##for py27\n",
    "zzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diab_model/n_ft_weight:0\n",
      "diab_model/n_ft_bias:0\n",
      "diab_model/weight:0\n",
      "diab_model/bias:0\n",
      "diab_model/RNN/RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix:0\n",
      "diab_model/RNN/RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias:0\n",
      "diab_model/RNN/RNN/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix:0\n",
      "diab_model/RNN/RNN/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias:0\n"
     ]
    }
   ],
   "source": [
    "for itm in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(itm.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create input data\n",
    "len1 = 4\n",
    "len2 = 1\n",
    "X = np.random.randn(2,len1,5)\n",
    "## set the length of the 2nd sample len2 = 1 (and pad zeros):\n",
    "\n",
    "X[1,len2:]=0\n",
    "X[1, :len2] = X[0, :len2]\n",
    "seq_len = np.array([len1, len2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create a LSTM cell:\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell( num_units=3, state_is_tuple=True  )\n",
    "#lstm_cell = tf.contrib.rnn.BasicLSTMCell( num_units=3, state_is_tuple=True )\n",
    "seq_lengths = tf.placeholder(dtype=tf.int32, shape=[2])\n",
    "## specify dtype when there's no initial states:\n",
    "outputs, last_states = tf.nn.dynamic_rnn( lstm_cell, X, seq_lengths, \n",
    "                                          dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables() )\n",
    "#sess.run(tf.global_variables_initializer() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out, state = sess.run([outputs, last_states ],feed_dict={seq_lengths:\n",
    "                                                        seq_len} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 4, 3), (2, 2, 3))\n"
     ]
    }
   ],
   "source": [
    "## LSTM has two latent states  c  and  h:\n",
    "print(out.shape, np.array(state).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm outputs and last h-state for seq 0:\n",
      "[[ 0.15113592  0.12003063 -0.00958839]\n",
      " [ 0.14691026  0.14576676 -0.32608086]\n",
      " [ 0.35864184  0.23916316 -0.23207145]\n",
      " [ 0.38370038  0.10601759 -0.24954055]]\n",
      "[ 0.38370038  0.10601759 -0.24954055]\n"
     ]
    }
   ],
   "source": [
    "print('lstm outputs and last h-state for seq 0:')\n",
    "print( out[0])\n",
    "print( state[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm outputs and last state for seq 1:\n",
      "[[ 0.15113592  0.12003063 -0.00958839]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "[ 0.15113592  0.12003063 -0.00958839]\n"
     ]
    }
   ],
   "source": [
    "print('lstm outputs and last state for seq 1:')\n",
    "print( out[1])\n",
    "print( state[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
